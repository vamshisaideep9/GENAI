{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 18:19:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.81                 Driver Version: 560.81         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   76C    P8              5W /   50W |     177MiB /   4096MiB |     15%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1916    C+G   C:\\Windows\\System32\\dwm.exe                 N/A      |\n",
      "|    0   N/A  N/A     12668    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     14900    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14936    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16280    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17712    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install transformers\n",
    "\n",
    "\n",
    "Hugging Face Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text classification: Assigning a category to a piece of text.\n",
    "   \n",
    "- Sentiment Analysis\n",
    "- Topic Classification\n",
    "- Spam detection\n",
    "\n",
    "```\n",
    "classifier = pipeline(\"text-classification\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Token Classification: Assigning labels to individual tokens in a sequence.\n",
    "\n",
    "- Named Entity Recognition (NER)\n",
    "- Part-of-speech tagging\n",
    "\n",
    "```\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Question Answering: Extracting an answer from a given context based on a question.\n",
    "   \n",
    "```\n",
    "question_answering = pipeline('question-answering')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Text Generation: Generating text based on a given prompt.\n",
    "\n",
    "- Language Modeling\n",
    "- Story Generation\n",
    "\n",
    "```\n",
    "text_generator = pipeline('text-generation')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Summarization: condensing long documents into shorter summaries.\n",
    "\n",
    "```\n",
    "summarizer = pipeliner(\"summarization\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Translation: Translating text from one language to another\n",
    "\n",
    "```\n",
    "translater = pipeline('Translation', model=\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Text2Text Generation: General Purpose text transformation, including summarization and translation.\n",
    "\n",
    "```\n",
    "text2text_generator = pipeline('text2text-generation')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Feature Extraction: Extracting hidden states or features from text.\n",
    "\n",
    "```\n",
    "feature_extractor = pipeline('feature-extraction')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. sentence_similarity: Measuring the similarity between two sentences.\n",
    "\n",
    "```\n",
    "sentence_similarity = pipeline('sentence-similarity')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer Vision Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Image Classification: classifying the main content of an image.\n",
    "\n",
    "```\n",
    "image_classifier = pipeline(\"image-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Object Detection: Identifying objects within an image and their bounding boxes.\n",
    "\n",
    "```\n",
    "object_detection = pipeline(\"object-detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Image segmentation: Segmenting different parts of an image into classes.\n",
    "\n",
    "```\n",
    "image_segmenter = pipeline(\"image-segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Image Generation: Generating images from textual descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Processing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Automatic Speech Recognition (ASR) : Converting spoken language into text.\n",
    "\n",
    "```\n",
    "speech_recognizer = pipeline(\"automatic-speech-recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Speech Translation: Translating spoken language from one language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Audio classification: Classifying audio signals into predefined categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-model Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Image Captioning: Generating a textual description of an image\n",
    "\n",
    "```\n",
    "image_captioner = pipeline(\"image-to-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visual Question Answering(VQA): Answering questions about the content of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Table Question Answering: Answering questions based on tabular data.\n",
    "\n",
    "```\n",
    "table_qa = pipeline(\"table-question-answering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Document question answering: Extracting answers from documents like PDFs\n",
    "\n",
    "```\n",
    "doc_qa = pipeline(\"document-question-answering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Time series Forecasting: Predicting Future Values in time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLP Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Should print 2.6.0\n",
    "print(torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "os.environ['HUGGINGFACE_API_KEY'] = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is PyTorch Available: True\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import is_torch_available\n",
    "\n",
    "print(\"Is PyTorch Available:\", is_torch_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vamsh\\OneDrive\\Desktop\\GENAI\\GENAI\\genai\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Fake', 'score': 0.8022446036338806}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"openai-community/roberta-base-openai-detector\")\n",
    "result = classifier(\"I was so not happy with the last Mission Impossible Movie\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.7693338394165039}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(task = \"sentiment-analysis\", model=\"facebook/bart-large-mnli\")\\\n",
    "                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n",
    "                                      Lots of them Looks very Promising. \\\n",
    "                                      I am not sure if we CAN actually Evaluate LLMs. \\\n",
    "                                      There is still lots to do.\\\n",
    "                                      Don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'admiration', 'score': 0.740653932094574},\n",
       " {'label': 'confusion', 'score': 0.9066852331161499},\n",
       " {'label': 'neutral', 'score': 0.7837758660316467},\n",
       " {'label': 'anger', 'score': 0.7870617508888245}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
    "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
    "            \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know.\",\\\n",
    "            \"I hate long Meetings.\"]\n",
    "classifier(task_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated_text:\n",
      "  Today is a rainy day in London and a snow-covered street is almost impassable.\n",
      "\n",
      "\n",
      "The police have been asked to provide a map to show how much of London had to bear by the time the event started and how much people\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")\n",
    "generated_text = text_generator(\"Today is a rainy day in London\",\n",
    "                                truncation=True,\n",
    "                                num_return_sequences = 2)\n",
    "print(\"Generated_text:\\n \", generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question Answering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.3205845057964325,\n",
       " 'start': 5,\n",
       " 'end': 25,\n",
       " 'answer': 'developing AI models'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model = pipeline(\"question-answering\")\n",
    "question = 'What is my job'\n",
    "context = \"I am developing AI models with python.\"\n",
    "qa_model(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '2 stars', 'score': 0.5099301934242249}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "model_name2 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "mymodel2 = AutoModelForSequenceClassification.from_pretrained(model_name2)\n",
    "mytokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model = mymodel2 , tokenizer = mytokenizer2)\n",
    "res = classifier(\"I was so not happy with the Barbie Movie\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'so', 'not', 'happy', 'with', 'the', 'bar', '##bie', 'movie']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#Load pretrained TOkenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "text = \"I was so not happy with the barbie movie\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146, 1108, 1177, 1136, 2816, 1114, 1103, 2927, 9725, 2523]\n"
     ]
    }
   ],
   "source": [
    "#convert tokens to input IDs\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input:  {'input_ids': [101, 146, 1108, 1177, 1136, 2816, 1114, 1103, 2927, 9725, 2523, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#encode the text (tokenization + converting to input IDs)\n",
    "\n",
    "encoded_input = tokenizer(text)\n",
    "print(\"Encoded Input: \", encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Output:  I was so not happy with the barbie movie\n"
     ]
    }
   ],
   "source": [
    "#Decode the text\n",
    "\n",
    "decoded_output = tokenizer.decode(input_ids)\n",
    "print(\"Decoded Output: \", decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **token_type_ids**:\n",
    "\n",
    "These ids are used to distinguish between different sequences in tasks that involve multiple sentences, such as question-answering and sentence-pair classification. BERT uses this mechanism to understand which tokens belong to which segment. For single-sequence tasks like sentiment analysis, token_type_ids are all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **attention_mask**:\n",
    "\n",
    "The attention mask is used to differentiate between actual tokens and padding tokens(if any). It helps the model focus on non-padding tokens and ignore padding tokens. A value 1 indicates that the token should be attended to, while a value 0 indicates padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why padding tokens are used**\n",
    "\n",
    "Uniform Sequence length: Deep Learning models typically process input data in batches.To efficiently process these batches, all sequences in a batch must have the same length. Padding tokens ensure this by extending shorter sequences to match the length of the longest sequence in the batch.\n",
    "\n",
    "Efficient COmputation: Fixed length sequences allow for more efficient use of hardware resources, as the model can process all sequences in parallel without needing to handle variable-length sequences individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FineTuning IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.13-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.5.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vamsh\\onedrive\\desktop\\genai\\genai\\cuda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached aiohttp-3.11.13-cp312-cp312-win_amd64.whl (438 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached propcache-0.3.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 attrs-25.1.0 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Maiden Voyage is just that. I\\'d like to say straight away that I watched 5mins of this before I just couldn\\'t stand it anymore. As already stated in another comment, this film doesn\\'t fall into the whole \"so bad it\\'s good\" thing, it\\'s just bad. The acting is awful, the sfx are poor, and the story is bland and stupid. Even the extras suck, the \"bag guy guards\" and such appear to hold their weapons like water pistols.<br /><br />Don\\'t even bother watching this film, the only thing special about it is that, no matter how low your expectations are, you will still be disappointed.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  146,\n",
       "  12765,\n",
       "  146,\n",
       "  6586,\n",
       "  140,\n",
       "  19556,\n",
       "  19368,\n",
       "  13329,\n",
       "  118,\n",
       "  162,\n",
       "  21678,\n",
       "  2162,\n",
       "  17056,\n",
       "  1121,\n",
       "  1139,\n",
       "  1888,\n",
       "  2984,\n",
       "  1272,\n",
       "  1104,\n",
       "  1155,\n",
       "  1103,\n",
       "  6392,\n",
       "  1115,\n",
       "  4405,\n",
       "  1122,\n",
       "  1165,\n",
       "  1122,\n",
       "  1108,\n",
       "  1148,\n",
       "  1308,\n",
       "  1107,\n",
       "  2573,\n",
       "  119,\n",
       "  146,\n",
       "  1145,\n",
       "  1767,\n",
       "  1115,\n",
       "  1120,\n",
       "  1148,\n",
       "  1122,\n",
       "  1108,\n",
       "  7842,\n",
       "  1118,\n",
       "  158,\n",
       "  119,\n",
       "  156,\n",
       "  119,\n",
       "  10148,\n",
       "  1191,\n",
       "  1122,\n",
       "  1518,\n",
       "  1793,\n",
       "  1106,\n",
       "  3873,\n",
       "  1142,\n",
       "  1583,\n",
       "  117,\n",
       "  3335,\n",
       "  1217,\n",
       "  170,\n",
       "  5442,\n",
       "  1104,\n",
       "  2441,\n",
       "  1737,\n",
       "  107,\n",
       "  6241,\n",
       "  107,\n",
       "  146,\n",
       "  1541,\n",
       "  1125,\n",
       "  1106,\n",
       "  1267,\n",
       "  1142,\n",
       "  1111,\n",
       "  1991,\n",
       "  119,\n",
       "  133,\n",
       "  9304,\n",
       "  120,\n",
       "  135,\n",
       "  133,\n",
       "  9304,\n",
       "  120,\n",
       "  135,\n",
       "  1109,\n",
       "  4928,\n",
       "  1110,\n",
       "  8663,\n",
       "  1213,\n",
       "  170,\n",
       "  1685,\n",
       "  3619,\n",
       "  3362,\n",
       "  2377,\n",
       "  1417,\n",
       "  14960,\n",
       "  1150,\n",
       "  3349,\n",
       "  1106,\n",
       "  3858,\n",
       "  1917,\n",
       "  1131,\n",
       "  1169,\n",
       "  1164,\n",
       "  1297,\n",
       "  119,\n",
       "  1130,\n",
       "  2440,\n",
       "  1131,\n",
       "  3349,\n",
       "  1106,\n",
       "  2817,\n",
       "  1123,\n",
       "  2209,\n",
       "  1116,\n",
       "  1106,\n",
       "  1543,\n",
       "  1199,\n",
       "  3271,\n",
       "  1104,\n",
       "  4148,\n",
       "  1113,\n",
       "  1184,\n",
       "  1103,\n",
       "  1903,\n",
       "  156,\n",
       "  11547,\n",
       "  1162,\n",
       "  1354,\n",
       "  1164,\n",
       "  2218,\n",
       "  1741,\n",
       "  2492,\n",
       "  1216,\n",
       "  1112,\n",
       "  1103,\n",
       "  4357,\n",
       "  1414,\n",
       "  1105,\n",
       "  1886,\n",
       "  2492,\n",
       "  1107,\n",
       "  1103,\n",
       "  1244,\n",
       "  1311,\n",
       "  119,\n",
       "  1130,\n",
       "  1206,\n",
       "  4107,\n",
       "  8673,\n",
       "  1105,\n",
       "  6655,\n",
       "  10552,\n",
       "  3708,\n",
       "  2316,\n",
       "  1104,\n",
       "  8583,\n",
       "  1164,\n",
       "  1147,\n",
       "  11089,\n",
       "  1113,\n",
       "  4039,\n",
       "  117,\n",
       "  1131,\n",
       "  1144,\n",
       "  2673,\n",
       "  1114,\n",
       "  1123,\n",
       "  3362,\n",
       "  3218,\n",
       "  117,\n",
       "  22150,\n",
       "  117,\n",
       "  1105,\n",
       "  1597,\n",
       "  1441,\n",
       "  119,\n",
       "  133,\n",
       "  9304,\n",
       "  120,\n",
       "  135,\n",
       "  133,\n",
       "  9304,\n",
       "  120,\n",
       "  135,\n",
       "  1327,\n",
       "  8567,\n",
       "  1143,\n",
       "  1164,\n",
       "  146,\n",
       "  6586,\n",
       "  140,\n",
       "  19556,\n",
       "  19368,\n",
       "  13329,\n",
       "  118,\n",
       "  162,\n",
       "  21678,\n",
       "  2162,\n",
       "  17056,\n",
       "  1110,\n",
       "  1115,\n",
       "  1969,\n",
       "  1201,\n",
       "  2403,\n",
       "  117,\n",
       "  1142,\n",
       "  1108,\n",
       "  1737,\n",
       "  185,\n",
       "  8456,\n",
       "  9597,\n",
       "  119,\n",
       "  8762,\n",
       "  117,\n",
       "  1103,\n",
       "  2673,\n",
       "  1105,\n",
       "  183,\n",
       "  17294,\n",
       "  2340,\n",
       "  4429,\n",
       "  1132,\n",
       "  1374,\n",
       "  1105,\n",
       "  1677,\n",
       "  1206,\n",
       "  117,\n",
       "  1256,\n",
       "  1173,\n",
       "  1122,\n",
       "  112,\n",
       "  188,\n",
       "  1136,\n",
       "  2046,\n",
       "  1176,\n",
       "  1199,\n",
       "  10928,\n",
       "  1193,\n",
       "  1189,\n",
       "  185,\n",
       "  8456,\n",
       "  1186,\n",
       "  119,\n",
       "  1799,\n",
       "  1139,\n",
       "  1583,\n",
       "  2354,\n",
       "  1713,\n",
       "  1525,\n",
       "  1122,\n",
       "  19196,\n",
       "  117,\n",
       "  1107,\n",
       "  3958,\n",
       "  2673,\n",
       "  1105,\n",
       "  183,\n",
       "  17294,\n",
       "  2340,\n",
       "  1132,\n",
       "  170,\n",
       "  1558,\n",
       "  22088,\n",
       "  1107,\n",
       "  3619,\n",
       "  7678,\n",
       "  119,\n",
       "  2431,\n",
       "  1130,\n",
       "  14721,\n",
       "  1197,\n",
       "  27644,\n",
       "  117,\n",
       "  18271,\n",
       "  1147,\n",
       "  2590,\n",
       "  1106,\n",
       "  1363,\n",
       "  1385,\n",
       "  2298,\n",
       "  1287,\n",
       "  4100,\n",
       "  117,\n",
       "  1125,\n",
       "  2673,\n",
       "  4429,\n",
       "  1107,\n",
       "  1117,\n",
       "  2441,\n",
       "  119,\n",
       "  133,\n",
       "  9304,\n",
       "  120,\n",
       "  135,\n",
       "  133,\n",
       "  9304,\n",
       "  120,\n",
       "  135,\n",
       "  146,\n",
       "  1202,\n",
       "  3254,\n",
       "  2354,\n",
       "  1181,\n",
       "  1103,\n",
       "  18992,\n",
       "  1111,\n",
       "  1103,\n",
       "  1864,\n",
       "  1115,\n",
       "  1251,\n",
       "  2673,\n",
       "  2602,\n",
       "  1107,\n",
       "  1103,\n",
       "  1273,\n",
       "  1110,\n",
       "  2602,\n",
       "  1111,\n",
       "  6037,\n",
       "  4998,\n",
       "  1897,\n",
       "  1190,\n",
       "  1198,\n",
       "  1106,\n",
       "  4900,\n",
       "  1234,\n",
       "  1105,\n",
       "  1294,\n",
       "  1948,\n",
       "  1106,\n",
       "  1129,\n",
       "  2602,\n",
       "  1107,\n",
       "  185,\n",
       "  8456,\n",
       "  9597,\n",
       "  13090,\n",
       "  1107,\n",
       "  1738,\n",
       "  119,\n",
       "  146,\n",
       "  6586,\n",
       "  140,\n",
       "  19556,\n",
       "  19368,\n",
       "  13329,\n",
       "  118,\n",
       "  162,\n",
       "  21678,\n",
       "  2162,\n",
       "  17056,\n",
       "  1110,\n",
       "  170,\n",
       "  1363,\n",
       "  1273,\n",
       "  1111,\n",
       "  2256,\n",
       "  5277,\n",
       "  1106,\n",
       "  2025,\n",
       "  1103,\n",
       "  6092,\n",
       "  1105,\n",
       "  15866,\n",
       "  113,\n",
       "  1185,\n",
       "  23609,\n",
       "  1179,\n",
       "  3005,\n",
       "  114,\n",
       "  1104,\n",
       "  3619,\n",
       "  7678,\n",
       "  119,\n",
       "  1252,\n",
       "  1541,\n",
       "  117,\n",
       "  1142,\n",
       "  1273,\n",
       "  2144,\n",
       "  112,\n",
       "  189,\n",
       "  1138,\n",
       "  1277,\n",
       "  1104,\n",
       "  170,\n",
       "  4928,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./results\\runs\\Mar11_20-16-09_ASUS,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=OptimizerNames.ADAMW_TORCH,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./results,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=16,\n",
       "per_device_train_batch_size=16,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./results,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=SaveStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('dslim/bert-base-NER')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/1563 04:54 < 18:11:27, 0.02 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./fine-tuned-model')\n",
    "tokenizer.save_pretrained('./fine-tuned-tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'ai OR artificial Intelligence or Machine Learning'\n",
    "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamsh\\AppData\\Local\\Temp\\ipykernel_15532\\1793932225.py:5: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-10 17:50:04+00:00</td>\n",
       "      <td>Robusto-1 Dataset: Comparing Humans and VLMs o...</td>\n",
       "      <td>As multimodal foundational models start being ...</td>\n",
       "      <td>[cs.CV, cs.AI, cs.RO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-10 17:46:59+00:00</td>\n",
       "      <td>Complexity Analysis of Environmental Time Series</td>\n",
       "      <td>Small, forested catchments are prototypes of t...</td>\n",
       "      <td>[nlin.CD, nlin.AO, physics.data-an, stat.ME]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-10 17:45:30+00:00</td>\n",
       "      <td>Neural Combinatorial Optimization via Preferen...</td>\n",
       "      <td>Neural Combinatorial Optimization (NCO) has em...</td>\n",
       "      <td>[cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-10 17:44:46+00:00</td>\n",
       "      <td>Denoising Score Distillation: From Noisy Diffu...</td>\n",
       "      <td>Diffusion models have achieved remarkable succ...</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-10 17:43:54+00:00</td>\n",
       "      <td>Joint Sampling Frequency Offset Estimation and...</td>\n",
       "      <td>This paper introduces a sampling frequency off...</td>\n",
       "      <td>[eess.SP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-03-10 17:40:43+00:00</td>\n",
       "      <td>Optimizing Test-Time Compute via Meta Reinforc...</td>\n",
       "      <td>Training models to effectively use test-time c...</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-03-10 17:40:05+00:00</td>\n",
       "      <td>Split-n-Chain: Privacy-Preserving Multi-Node S...</td>\n",
       "      <td>Deep learning, when integrated with a large am...</td>\n",
       "      <td>[cs.CR, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-03-10 17:38:42+00:00</td>\n",
       "      <td>Runtime Detection of Adversarial Attacks in AI...</td>\n",
       "      <td>Rapid adoption of AI technologies raises sever...</td>\n",
       "      <td>[cs.CR, cs.AI, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-03-10 17:37:56+00:00</td>\n",
       "      <td>A Universally Optimal Primal-Dual Method for M...</td>\n",
       "      <td>This paper proposes a universal, optimal algor...</td>\n",
       "      <td>[math.OC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-03-10 17:37:39+00:00</td>\n",
       "      <td>Inductive Moment Matching</td>\n",
       "      <td>Diffusion models and Flow Matching generate hi...</td>\n",
       "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published  \\\n",
       "0 2025-03-10 17:50:04+00:00   \n",
       "1 2025-03-10 17:46:59+00:00   \n",
       "2 2025-03-10 17:45:30+00:00   \n",
       "3 2025-03-10 17:44:46+00:00   \n",
       "4 2025-03-10 17:43:54+00:00   \n",
       "5 2025-03-10 17:40:43+00:00   \n",
       "6 2025-03-10 17:40:05+00:00   \n",
       "7 2025-03-10 17:38:42+00:00   \n",
       "8 2025-03-10 17:37:56+00:00   \n",
       "9 2025-03-10 17:37:39+00:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Robusto-1 Dataset: Comparing Humans and VLMs o...   \n",
       "1   Complexity Analysis of Environmental Time Series   \n",
       "2  Neural Combinatorial Optimization via Preferen...   \n",
       "3  Denoising Score Distillation: From Noisy Diffu...   \n",
       "4  Joint Sampling Frequency Offset Estimation and...   \n",
       "5  Optimizing Test-Time Compute via Meta Reinforc...   \n",
       "6  Split-n-Chain: Privacy-Preserving Multi-Node S...   \n",
       "7  Runtime Detection of Adversarial Attacks in AI...   \n",
       "8  A Universally Optimal Primal-Dual Method for M...   \n",
       "9                          Inductive Moment Matching   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  As multimodal foundational models start being ...   \n",
       "1  Small, forested catchments are prototypes of t...   \n",
       "2  Neural Combinatorial Optimization (NCO) has em...   \n",
       "3  Diffusion models have achieved remarkable succ...   \n",
       "4  This paper introduces a sampling frequency off...   \n",
       "5  Training models to effectively use test-time c...   \n",
       "6  Deep learning, when integrated with a large am...   \n",
       "7  Rapid adoption of AI technologies raises sever...   \n",
       "8  This paper proposes a universal, optimal algor...   \n",
       "9  Diffusion models and Flow Matching generate hi...   \n",
       "\n",
       "                                     categories  \n",
       "0                         [cs.CV, cs.AI, cs.RO]  \n",
       "1  [nlin.CD, nlin.AO, physics.data-an, stat.ME]  \n",
       "2                                       [cs.LG]  \n",
       "3                         [cs.LG, cs.AI, cs.CV]  \n",
       "4                                     [eess.SP]  \n",
       "5                         [cs.LG, cs.AI, cs.CL]  \n",
       "6                                [cs.CR, cs.LG]  \n",
       "7                         [cs.CR, cs.AI, cs.LG]  \n",
       "8                                     [math.OC]  \n",
       "9                       [cs.LG, cs.AI, stat.ML]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetch papers\n",
    "\n",
    "papers = []\n",
    "\n",
    "for result in search.results():\n",
    "    papers.append({\n",
    "        'published': result.published,\n",
    "        'title': result.title,\n",
    "        'abstract': result.summary,\n",
    "        'categories': result.categories\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-10 17:50:04+00:00</td>\n",
       "      <td>Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru</td>\n",
       "      <td>As multimodal foundational models start being deployed experimentally in\\nSelf-Driving cars, a reasonable question we ask ourselves is how similar to\\nhumans do these systems respond in certain driving situations -- especially\\nthose that are out-of-distribution? To study this, we create the Robusto-1\\ndataset that uses dashcam video data from Peru, a country with one of the worst\\n(aggressive) drivers in the world, a high traffic index, and a high ratio of\\nbizarre to non-bizarre street objects likely never seen in training. In\\nparticular, to preliminarly test at a cognitive level how well Foundational\\nVisual Language Models (VLMs) compare to Humans in Driving, we move away from\\nbounding boxes, segmentation maps, occupancy maps or trajectory estimation to\\nmulti-modal Visual Question Answering (VQA) comparing both humans and machines\\nthrough a popular method in systems neuroscience known as Representational\\nSimilarity Analysis (RSA). Depending on the type of questions we ask and the\\nanswers these systems give, we will show in what cases do VLMs and Humans\\nconverge or diverge allowing us to probe on their cognitive alignment. We find\\nthat the degree of alignment varies significantly depending on the type of\\nquestions asked to each type of system (Humans vs VLMs), highlighting a gap in\\ntheir alignment.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.RO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-10 17:46:59+00:00</td>\n",
       "      <td>Complexity Analysis of Environmental Time Series</td>\n",
       "      <td>Small, forested catchments are prototypes of terrestrial ecosystems and have\\nbeen studied in several disciplines of environmental sciences since several\\ndecades. Time series of water and matter fluxes and nutrient concentrations\\nfrom these systems exhibit a bewildering diversity of spatio-temporal patterns,\\nindicating the intricate nature of processes acting on a large range of time\\nscales. Nonlinear dynamics is an obvious framework to investigate catchment\\ntime series. We analyze selected long-term data from three headwater catchments\\nin the Bramke valley, Harz mountains, Lower Saxony in Germany at common\\nbiweekly resolution for the period 1991 to 2023. For every time series, we\\nperform gap filling, detrending and removal of the annual cycle using Singular\\nSystem Analysis (SSA), and then calculate metrics based on ordinal pattern\\nstatistics: the permutation entropy, permutation complexity and Fisher\\ninformation, as well as their generalized versions (q-entropy and\\n{\\alpha}-entropy). Further, the position of each variable in Tarnopolski\\ndiagrams is displayed and compared to reference stochastic processes, like\\nfractional Brownian motion, fractional Gaussian noise, and \\b{eta} noise. Still\\nanother way of distinguishing deterministic chaos and structured noise, and\\nquantifying the latter, is provided by the complexity from ordinal pattern\\npositioned slopes (COPPS). We also construct Horizontal Visibility Graphs and\\nestimate the exponent of the decay of the degree distribution. Taken together,\\nthe analyses create a characterization of the dynamics of these systems which\\ncan be scrutinized for universality, either across variables or between the\\nthree geographically very close catchments.</td>\n",
       "      <td>[nlin.CD, nlin.AO, physics.data-an, stat.ME]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-10 17:45:30+00:00</td>\n",
       "      <td>Neural Combinatorial Optimization via Preference Optimization</td>\n",
       "      <td>Neural Combinatorial Optimization (NCO) has emerged as a promising approach\\nfor NP-hard problems. However, prevailing RL-based methods suffer from low\\nsample efficiency due to sparse rewards and underused solutions. We propose\\nPreference Optimization for Combinatorial Optimization (POCO), a training\\nparadigm that leverages solution preferences via objective values. It\\nintroduces: (1) an efficient preference pair construction for better explore\\nand exploit solutions, and (2) a novel loss function that adaptively scales\\ngradients via objective differences, removing reliance on reward models or\\nreference policies. Experiments on Job-Shop Scheduling (JSP), Traveling\\nSalesman (TSP), and Flexible Job-Shop Scheduling (FJSP) show POCO outperforms\\nstate-of-the-art neural methods, reducing optimality gaps impressively with\\nefficient inference. POCO is architecture-agnostic, enabling seamless\\nintegration with existing NCO models, and establishes preference optimization\\nas a principled framework for combinatorial optimization.</td>\n",
       "      <td>[cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-10 17:44:46+00:00</td>\n",
       "      <td>Denoising Score Distillation: From Noisy Diffusion Pretraining to One-Step High-Quality Generation</td>\n",
       "      <td>Diffusion models have achieved remarkable success in generating\\nhigh-resolution, realistic images across diverse natural distributions.\\nHowever, their performance heavily relies on high-quality training data, making\\nit challenging to learn meaningful distributions from corrupted samples. This\\nlimitation restricts their applicability in scientific domains where clean data\\nis scarce or costly to obtain. In this work, we introduce denoising score\\ndistillation (DSD), a surprisingly effective and novel approach for training\\nhigh-quality generative models from low-quality data. DSD first pretrains a\\ndiffusion model exclusively on noisy, corrupted samples and then distills it\\ninto a one-step generator capable of producing refined, clean outputs. While\\nscore distillation is traditionally viewed as a method to accelerate diffusion\\nmodels, we show that it can also significantly enhance sample quality,\\nparticularly when starting from a degraded teacher model. Across varying noise\\nlevels and datasets, DSD consistently improves generative performancewe\\nsummarize our empirical evidence in Fig. 1. Furthermore, we provide theoretical\\ninsights showing that, in a linear model setting, DSD identifies the eigenspace\\nof the clean data distributions covariance matrix, implicitly regularizing the\\ngenerator. This perspective reframes score distillation as not only a tool for\\nefficiency but also a mechanism for improving generative models, particularly\\nin low-quality data settings.</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-10 17:43:54+00:00</td>\n",
       "      <td>Joint Sampling Frequency Offset Estimation and Compensation Based on the Farrow Structure</td>\n",
       "      <td>This paper introduces a sampling frequency offset (SFO) estimation method\\nbased on the Farrow structure, which is typically utilized for the SFO\\ncompensation and thereby enables a reduction of the implementation complexity\\nof the SFO estimation. The proposed method is implemented in the time domain\\nand works for arbitrary bandlimited signals, thus with no additional\\nconstraints on the waveform structure. Moreover, it can operate on only the\\nreal or imaginary part of a complex signal, which further reduces the\\nestimation complexity. Furthermore, the proposed method can simultaneously\\nestimate the SFO and additional sampling time offset (STO) and it is\\ninsensitive to other synchronization errors, like carrier frequency offset.\\nBoth the derivations of the proposed method and its implementation are\\npresented, and through simulation examples, it is demonstrated that it can\\naccurately estimate both SFO and STO for different types of bandlimited\\nsignals.</td>\n",
       "      <td>[eess.SP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-03-10 17:40:43+00:00</td>\n",
       "      <td>Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning</td>\n",
       "      <td>Training models to effectively use test-time compute is crucial for improving\\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\\non search traces or running RL with 0/1 outcome reward, but do these approaches\\nefficiently utilize test-time compute? Would these approaches continue to scale\\nas the budget improves? In this paper, we try to answer these questions. We\\nformalize the problem of optimizing test-time compute as a meta-reinforcement\\nlearning (RL) problem, which provides a principled perspective on spending\\ntest-time compute. This perspective enables us to view the long output stream\\nfrom the LLM as consisting of several episodes run at test time and leads us to\\nuse a notion of cumulative regret over output tokens as a way to measure the\\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\\nexploration and exploitation over training, minimizing cumulative regret would\\nalso provide the best balance between exploration and exploitation in the token\\nstream. While we show that state-of-the-art models do not minimize regret, one\\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\\nthe output stream, quantified by the change in the likelihood of eventual\\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\\nefficiency for math reasoning compared to outcome-reward RL.</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-03-10 17:40:05+00:00</td>\n",
       "      <td>Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with Blockchain-Based Auditability</td>\n",
       "      <td>Deep learning, when integrated with a large amount of training data, has the\\npotential to outperform machine learning in terms of high accuracy. Recently,\\nprivacy-preserving deep learning has drawn significant attention of the\\nresearch community. Different privacy notions in deep learning include privacy\\nof data provided by data-owners and privacy of parameters and/or\\nhyperparameters of the underlying neural network. Federated learning is a\\npopular privacy-preserving execution environment where data-owners participate\\nin learning the parameters collectively without leaking their respective data\\nto other participants. However, federated learning suffers from certain\\nsecurity/privacy issues. In this paper, we propose Split-n-Chain, a variant of\\nsplit learning where the layers of the network are split among several\\ndistributed nodes. Split-n-Chain achieves several privacy properties:\\ndata-owners need not share their training data with other nodes, and no nodes\\nhave access to the parameters and hyperparameters of the neural network (except\\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\\nblockchain to audit the computation done by different nodes. Our experimental\\nresults show that: Split-n-Chain is efficient, in terms of time required to\\nexecute different phases, and the training loss trend is similar to that for\\nthe same neural network when implemented in a monolithic fashion.</td>\n",
       "      <td>[cs.CR, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-03-10 17:38:42+00:00</td>\n",
       "      <td>Runtime Detection of Adversarial Attacks in AI Accelerators Using Performance Counters</td>\n",
       "      <td>Rapid adoption of AI technologies raises several major security concerns,\\nincluding the risks of adversarial perturbations, which threaten the\\nconfidentiality and integrity of AI applications. Protecting AI hardware from\\nmisuse and diverse security threats is a challenging task. To address this\\nchallenge, we propose SAMURAI, a novel framework for safeguarding against\\nmalicious usage of AI hardware and its resilience to attacks. SAMURAI\\nintroduces an AI Performance Counter (APC) for tracking dynamic behavior of an\\nAI model coupled with an on-chip Machine Learning (ML) analysis engine, known\\nas TANTO (Trained Anomaly Inspection Through Trace Observation). APC records\\nthe runtime profile of the low-level hardware events of different AI\\noperations. Subsequently, the summary information recorded by the APC is\\nprocessed by TANTO to efficiently identify potential security breaches and\\nensure secure, responsible use of AI. SAMURAI enables real-time detection of\\nsecurity threats and misuse without relying on traditional software-based\\nsolutions that require model integration. Experimental results demonstrate that\\nSAMURAI achieves up to 97% accuracy in detecting adversarial attacks with\\nmoderate overhead on various AI models, significantly outperforming\\nconventional software-based approaches. It enhances security and regulatory\\ncompliance, providing a comprehensive solution for safeguarding AI against\\nemergent threats.</td>\n",
       "      <td>[cs.CR, cs.AI, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-03-10 17:37:56+00:00</td>\n",
       "      <td>A Universally Optimal Primal-Dual Method for Minimizing Heterogeneous Compositions</td>\n",
       "      <td>This paper proposes a universal, optimal algorithm for convex minimization\\nproblems of the composite form $g_0(x)+h(g_1(x),\\dots, g_m(x)) + u(x)$. We\\nallow each $g_j$ to independently range from being nonsmooth Lipschitz to\\nsmooth, from convex to strongly convex, described by notions of H\\\"older\\ncontinuous gradients and uniform convexity. Note that, although the objective\\nis built from a heterogeneous combination of such structured components, it\\ndoes not necessarily possess smoothness, Lipschitzness, or any favorable\\nstructure overall other than convexity. Regardless, we provide a universal\\noptimal method in terms of oracle access to (sub)gradients of each $g_j$. The\\nkey insight enabling our optimal universal analysis is the construction of two\\nnew constants, the Approximate Dualized Aggregate smoothness and strong\\nconvexity, which combine the benefits of each heterogeneous structure into\\nsingle quantities amenable to analysis. As a key application, fixing $h$ as the\\nnonpositive indicator function, this model readily captures functionally\\nconstrained minimization $g_0(x)+u(x)$ subject to $g_j(x)\\leq 0$. In\\nparticular, our algorithm and analysis are directly inspired by the smooth\\nconstrained minimization method of Zhang and Lan and consequently recover and\\ngeneralize their accelerated guarantees.</td>\n",
       "      <td>[math.OC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-03-10 17:37:39+00:00</td>\n",
       "      <td>Inductive Moment Matching</td>\n",
       "      <td>Diffusion models and Flow Matching generate high-quality samples but are slow\\nat inference, and distilling them into few-step models often leads to\\ninstability and extensive tuning. To resolve these trade-offs, we propose\\nInductive Moment Matching (IMM), a new class of generative models for one- or\\nfew-step sampling with a single-stage training procedure. Unlike distillation,\\nIMM does not require pre-training initialization and optimization of two\\nnetworks; and unlike Consistency Models, IMM guarantees distribution-level\\nconvergence and remains stable under various hyperparameters and standard model\\narchitectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID\\nusing only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98\\non CIFAR-10 for a model trained from scratch.</td>\n",
       "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published  \\\n",
       "0 2025-03-10 17:50:04+00:00   \n",
       "1 2025-03-10 17:46:59+00:00   \n",
       "2 2025-03-10 17:45:30+00:00   \n",
       "3 2025-03-10 17:44:46+00:00   \n",
       "4 2025-03-10 17:43:54+00:00   \n",
       "5 2025-03-10 17:40:43+00:00   \n",
       "6 2025-03-10 17:40:05+00:00   \n",
       "7 2025-03-10 17:38:42+00:00   \n",
       "8 2025-03-10 17:37:56+00:00   \n",
       "9 2025-03-10 17:37:39+00:00   \n",
       "\n",
       "                                                                                                       title  \\\n",
       "0  Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru   \n",
       "1                                                           Complexity Analysis of Environmental Time Series   \n",
       "2                                              Neural Combinatorial Optimization via Preference Optimization   \n",
       "3         Denoising Score Distillation: From Noisy Diffusion Pretraining to One-Step High-Quality Generation   \n",
       "4                  Joint Sampling Frequency Offset Estimation and Compensation Based on the Farrow Structure   \n",
       "5                                            Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning   \n",
       "6             Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with Blockchain-Based Auditability   \n",
       "7                     Runtime Detection of Adversarial Attacks in AI Accelerators Using Performance Counters   \n",
       "8                         A Universally Optimal Primal-Dual Method for Minimizing Heterogeneous Compositions   \n",
       "9                                                                                  Inductive Moment Matching   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  abstract  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                     As multimodal foundational models start being deployed experimentally in\\nSelf-Driving cars, a reasonable question we ask ourselves is how similar to\\nhumans do these systems respond in certain driving situations -- especially\\nthose that are out-of-distribution? To study this, we create the Robusto-1\\ndataset that uses dashcam video data from Peru, a country with one of the worst\\n(aggressive) drivers in the world, a high traffic index, and a high ratio of\\nbizarre to non-bizarre street objects likely never seen in training. In\\nparticular, to preliminarly test at a cognitive level how well Foundational\\nVisual Language Models (VLMs) compare to Humans in Driving, we move away from\\nbounding boxes, segmentation maps, occupancy maps or trajectory estimation to\\nmulti-modal Visual Question Answering (VQA) comparing both humans and machines\\nthrough a popular method in systems neuroscience known as Representational\\nSimilarity Analysis (RSA). Depending on the type of questions we ask and the\\nanswers these systems give, we will show in what cases do VLMs and Humans\\nconverge or diverge allowing us to probe on their cognitive alignment. We find\\nthat the degree of alignment varies significantly depending on the type of\\nquestions asked to each type of system (Humans vs VLMs), highlighting a gap in\\ntheir alignment.   \n",
       "1  Small, forested catchments are prototypes of terrestrial ecosystems and have\\nbeen studied in several disciplines of environmental sciences since several\\ndecades. Time series of water and matter fluxes and nutrient concentrations\\nfrom these systems exhibit a bewildering diversity of spatio-temporal patterns,\\nindicating the intricate nature of processes acting on a large range of time\\nscales. Nonlinear dynamics is an obvious framework to investigate catchment\\ntime series. We analyze selected long-term data from three headwater catchments\\nin the Bramke valley, Harz mountains, Lower Saxony in Germany at common\\nbiweekly resolution for the period 1991 to 2023. For every time series, we\\nperform gap filling, detrending and removal of the annual cycle using Singular\\nSystem Analysis (SSA), and then calculate metrics based on ordinal pattern\\nstatistics: the permutation entropy, permutation complexity and Fisher\\ninformation, as well as their generalized versions (q-entropy and\\n{\\alpha}-entropy). Further, the position of each variable in Tarnopolski\\ndiagrams is displayed and compared to reference stochastic processes, like\\nfractional Brownian motion, fractional Gaussian noise, and \\b{eta} noise. Still\\nanother way of distinguishing deterministic chaos and structured noise, and\\nquantifying the latter, is provided by the complexity from ordinal pattern\\npositioned slopes (COPPS). We also construct Horizontal Visibility Graphs and\\nestimate the exponent of the decay of the degree distribution. Taken together,\\nthe analyses create a characterization of the dynamics of these systems which\\ncan be scrutinized for universality, either across variables or between the\\nthree geographically very close catchments.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Neural Combinatorial Optimization (NCO) has emerged as a promising approach\\nfor NP-hard problems. However, prevailing RL-based methods suffer from low\\nsample efficiency due to sparse rewards and underused solutions. We propose\\nPreference Optimization for Combinatorial Optimization (POCO), a training\\nparadigm that leverages solution preferences via objective values. It\\nintroduces: (1) an efficient preference pair construction for better explore\\nand exploit solutions, and (2) a novel loss function that adaptively scales\\ngradients via objective differences, removing reliance on reward models or\\nreference policies. Experiments on Job-Shop Scheduling (JSP), Traveling\\nSalesman (TSP), and Flexible Job-Shop Scheduling (FJSP) show POCO outperforms\\nstate-of-the-art neural methods, reducing optimality gaps impressively with\\nefficient inference. POCO is architecture-agnostic, enabling seamless\\nintegration with existing NCO models, and establishes preference optimization\\nas a principled framework for combinatorial optimization.   \n",
       "3                                                                                                                                                                                                                                             Diffusion models have achieved remarkable success in generating\\nhigh-resolution, realistic images across diverse natural distributions.\\nHowever, their performance heavily relies on high-quality training data, making\\nit challenging to learn meaningful distributions from corrupted samples. This\\nlimitation restricts their applicability in scientific domains where clean data\\nis scarce or costly to obtain. In this work, we introduce denoising score\\ndistillation (DSD), a surprisingly effective and novel approach for training\\nhigh-quality generative models from low-quality data. DSD first pretrains a\\ndiffusion model exclusively on noisy, corrupted samples and then distills it\\ninto a one-step generator capable of producing refined, clean outputs. While\\nscore distillation is traditionally viewed as a method to accelerate diffusion\\nmodels, we show that it can also significantly enhance sample quality,\\nparticularly when starting from a degraded teacher model. Across varying noise\\nlevels and datasets, DSD consistently improves generative performancewe\\nsummarize our empirical evidence in Fig. 1. Furthermore, we provide theoretical\\ninsights showing that, in a linear model setting, DSD identifies the eigenspace\\nof the clean data distributions covariance matrix, implicitly regularizing the\\ngenerator. This perspective reframes score distillation as not only a tool for\\nefficiency but also a mechanism for improving generative models, particularly\\nin low-quality data settings.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This paper introduces a sampling frequency offset (SFO) estimation method\\nbased on the Farrow structure, which is typically utilized for the SFO\\ncompensation and thereby enables a reduction of the implementation complexity\\nof the SFO estimation. The proposed method is implemented in the time domain\\nand works for arbitrary bandlimited signals, thus with no additional\\nconstraints on the waveform structure. Moreover, it can operate on only the\\nreal or imaginary part of a complex signal, which further reduces the\\nestimation complexity. Furthermore, the proposed method can simultaneously\\nestimate the SFO and additional sampling time offset (STO) and it is\\ninsensitive to other synchronization errors, like carrier frequency offset.\\nBoth the derivations of the proposed method and its implementation are\\npresented, and through simulation examples, it is demonstrated that it can\\naccurately estimate both SFO and STO for different types of bandlimited\\nsignals.   \n",
       "5                                                                                                   Training models to effectively use test-time compute is crucial for improving\\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\\non search traces or running RL with 0/1 outcome reward, but do these approaches\\nefficiently utilize test-time compute? Would these approaches continue to scale\\nas the budget improves? In this paper, we try to answer these questions. We\\nformalize the problem of optimizing test-time compute as a meta-reinforcement\\nlearning (RL) problem, which provides a principled perspective on spending\\ntest-time compute. This perspective enables us to view the long output stream\\nfrom the LLM as consisting of several episodes run at test time and leads us to\\nuse a notion of cumulative regret over output tokens as a way to measure the\\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\\nexploration and exploitation over training, minimizing cumulative regret would\\nalso provide the best balance between exploration and exploitation in the token\\nstream. While we show that state-of-the-art models do not minimize regret, one\\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\\nthe output stream, quantified by the change in the likelihood of eventual\\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\\nefficiency for math reasoning compared to outcome-reward RL.   \n",
       "6                                                                                                                                                                                                                                                                                                            Deep learning, when integrated with a large amount of training data, has the\\npotential to outperform machine learning in terms of high accuracy. Recently,\\nprivacy-preserving deep learning has drawn significant attention of the\\nresearch community. Different privacy notions in deep learning include privacy\\nof data provided by data-owners and privacy of parameters and/or\\nhyperparameters of the underlying neural network. Federated learning is a\\npopular privacy-preserving execution environment where data-owners participate\\nin learning the parameters collectively without leaking their respective data\\nto other participants. However, federated learning suffers from certain\\nsecurity/privacy issues. In this paper, we propose Split-n-Chain, a variant of\\nsplit learning where the layers of the network are split among several\\ndistributed nodes. Split-n-Chain achieves several privacy properties:\\ndata-owners need not share their training data with other nodes, and no nodes\\nhave access to the parameters and hyperparameters of the neural network (except\\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\\nblockchain to audit the computation done by different nodes. Our experimental\\nresults show that: Split-n-Chain is efficient, in terms of time required to\\nexecute different phases, and the training loss trend is similar to that for\\nthe same neural network when implemented in a monolithic fashion.   \n",
       "7                                                                                                                                                                                                                                                                                              Rapid adoption of AI technologies raises several major security concerns,\\nincluding the risks of adversarial perturbations, which threaten the\\nconfidentiality and integrity of AI applications. Protecting AI hardware from\\nmisuse and diverse security threats is a challenging task. To address this\\nchallenge, we propose SAMURAI, a novel framework for safeguarding against\\nmalicious usage of AI hardware and its resilience to attacks. SAMURAI\\nintroduces an AI Performance Counter (APC) for tracking dynamic behavior of an\\nAI model coupled with an on-chip Machine Learning (ML) analysis engine, known\\nas TANTO (Trained Anomaly Inspection Through Trace Observation). APC records\\nthe runtime profile of the low-level hardware events of different AI\\noperations. Subsequently, the summary information recorded by the APC is\\nprocessed by TANTO to efficiently identify potential security breaches and\\nensure secure, responsible use of AI. SAMURAI enables real-time detection of\\nsecurity threats and misuse without relying on traditional software-based\\nsolutions that require model integration. Experimental results demonstrate that\\nSAMURAI achieves up to 97% accuracy in detecting adversarial attacks with\\nmoderate overhead on various AI models, significantly outperforming\\nconventional software-based approaches. It enhances security and regulatory\\ncompliance, providing a comprehensive solution for safeguarding AI against\\nemergent threats.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                  This paper proposes a universal, optimal algorithm for convex minimization\\nproblems of the composite form $g_0(x)+h(g_1(x),\\dots, g_m(x)) + u(x)$. We\\nallow each $g_j$ to independently range from being nonsmooth Lipschitz to\\nsmooth, from convex to strongly convex, described by notions of H\\\"older\\ncontinuous gradients and uniform convexity. Note that, although the objective\\nis built from a heterogeneous combination of such structured components, it\\ndoes not necessarily possess smoothness, Lipschitzness, or any favorable\\nstructure overall other than convexity. Regardless, we provide a universal\\noptimal method in terms of oracle access to (sub)gradients of each $g_j$. The\\nkey insight enabling our optimal universal analysis is the construction of two\\nnew constants, the Approximate Dualized Aggregate smoothness and strong\\nconvexity, which combine the benefits of each heterogeneous structure into\\nsingle quantities amenable to analysis. As a key application, fixing $h$ as the\\nnonpositive indicator function, this model readily captures functionally\\nconstrained minimization $g_0(x)+u(x)$ subject to $g_j(x)\\leq 0$. In\\nparticular, our algorithm and analysis are directly inspired by the smooth\\nconstrained minimization method of Zhang and Lan and consequently recover and\\ngeneralize their accelerated guarantees.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Diffusion models and Flow Matching generate high-quality samples but are slow\\nat inference, and distilling them into few-step models often leads to\\ninstability and extensive tuning. To resolve these trade-offs, we propose\\nInductive Moment Matching (IMM), a new class of generative models for one- or\\nfew-step sampling with a single-stage training procedure. Unlike distillation,\\nIMM does not require pre-training initialization and optimization of two\\nnetworks; and unlike Consistency Models, IMM guarantees distribution-level\\nconvergence and remains stable under various hyperparameters and standard model\\narchitectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID\\nusing only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98\\non CIFAR-10 for a model trained from scratch.   \n",
       "\n",
       "                                     categories  \n",
       "0                         [cs.CV, cs.AI, cs.RO]  \n",
       "1  [nlin.CD, nlin.AO, physics.data-an, stat.ME]  \n",
       "2                                       [cs.LG]  \n",
       "3                         [cs.LG, cs.AI, cs.CV]  \n",
       "4                                     [eess.SP]  \n",
       "5                         [cs.LG, cs.AI, cs.CL]  \n",
       "6                                [cs.CR, cs.LG]  \n",
       "7                         [cs.CR, cs.AI, cs.LG]  \n",
       "8                                     [math.OC]  \n",
       "9                       [cs.LG, cs.AI, stat.ML]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = df['abstract'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summarization_result = summarizer(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Robusto-1dataset uses dashcam video data from Peru, a country with one of the worst aggressive drivers in the world. The degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_result[0]['summary_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
